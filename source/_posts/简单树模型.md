---
title: （三）树模型(Decision Trees)
date: 2019-12-05 19:17:38
tags: [树, 决策树, 回归树, Decision Tree]
mathjax: true
---

## 树模型简介

树模型(Tree Model)或者说决策树模型(Decision Tree Model)可能是大家在入门机器学习时接触的最早的模型。它在监督学习(supervised learning) 的众多模型中，可以说是最符合人类思维的一种模型：通过一系列的“提问”，最终将这一样本点(sample)归到某个样本集合中去，再通过这一样本集合的共性特征去定义这一样本点的标签(label)。笔者记得很多年前，网络上流行这样的应用：你心中想一个名人明星，系统会给你提问若干问题，你只需要回答“是”、“不是”或者“不知道”即可，经过10+轮的提问，系统能够告诉你心之所想。其实，这个应用的背后就是一个针对名人明星的决策树，只不过这棵决策树不需要泛化能力，完全拟合训练集（名人明星样本）即可。

上述提到的针对样本的“一系列提问”，实际上做的是将整个特征空间(feature space)划分成一个个不相交的矩形(rectangles)。更准确地说，在二维特征空间中是不相交的矩形（如下图1[^1]所示），而在高维空间中是不相交的高维立方体。将样本点归到这些不相交的集合的过程，如下图所示，可以用一种树形的方式描述，这也是模型被称为“树模型”的原因。

{% grouppicture 2-2 %}
  !["树模型的树状描述结构"](/images/trees/tree1.png)
  !["树模型的特征空间划分"](/images/trees/tree2.png)
{% endgrouppicture %}
<div class="image-caption" style="margin: 6 auto;">
  <span class="image-caption" style="margin: 4 auto ">图1：树模型的树状结构描述及特征空间划分</span>
</div>

图2[^1]表示由决策树将`特征空间`划分为不相交的集合且在不同的特征空间子集上的取值，即模型的预测值。如果我们将整个`特征空间`用$\mathcal{R}$来表示，那么决策树会将整个空间划分为$\{\mathcal{R}_1, \mathcal{R}_2, ... , \mathcal{R}_m\}$，其中任意$i\neq j$且$i,j \in {1, ..., m}$满足$\mathcal{R}_i\bigcap\mathcal{R}_j=\emptyset$。

<img src="/images/trees/tree3.png" width="50%" height="50%" border="0" style="margin: 0 auto;"/>
<div class="image-caption" style="margin: 6 auto;">
  <span class="image-caption" style="margin: 4 auto ">图2：树模型特征空间划分及其对应值</span>
</div>

传统的机器学习参考书在介绍树模型时，一般采用的是列举`ID3`、`C4.5`和`CART`这几种树模型的算法。但是，笔者觉得看过一遍这几个算法的介绍，一直很难产生共鸣（这可能也与笔者之前没有真正动手实现一个树模型有关）。在深入阅读参考文献[1]和[3]后，笔者有了更深的感悟，这几种树模型其实核心的逻辑都很类似：我们没有办法暴力穷举去探索所有的`特征空间`的划分，只能够采用贪心策略(greedy strategy)寻找当下的一个最好的划分，而一旦我们划分好`特征空间`，确定每个特征子集合$\mathcal{R}_i$的值的方式都是近似的，这一点将在本文详细介绍。基于这样的认识，笔者认为也许对其中一种树模型详细探讨也许会有更多收获。这里我同参考文献[1]和[3]一样，选择`CART`算法进行详细介绍。工业界现在使用树模型很少会直接使用单棵的决策树拟合模型，而是会采用`Boosting`或者`Bagging`的方法，将很多很多树的结果“综合”一下，以得到更好的预测效果。`CART`的简洁以及既可以用于分类亦可用于回归也让`CART`成为`Boosting`和`Bagging`里广泛使用的基模型[^2]，所以也算是应用最广泛的树模型。

[^1]: 图片摘自参考文献[1]中章节9.2 “Tree-Based Model”。
[^2]: 基模型的概念后续会介绍，这里结合上下文可以理解为“很多树”中的一棵。

## 分类与回归树（Classfication & Regression Tree, CART, C&RT）

### CART中的分类

### CART中的回归

### CART里的细节

## CART与ID3、C4.5的对比

## 参考文献

[1]: Friedman J, Hastie T, Tibshirani R. The elements of statistical learning[M]. New York: Springer series in statistics, 2001.
[2]: 李航. 统计学习方法[J]. 2019.
[3]: David S. Rosenberg. https://bloomberg.github.io
[4]: 诸葛越, 葫芦娃. 百面机器学习——算法工程师带你去面试[J]. 2018.
