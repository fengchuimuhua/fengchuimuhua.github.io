---
title: （三）树模型(Decision Trees)
date: 2019-12-05 19:17:38
tags: [树, 决策树, 回归树, Decision Tree]
mathjax: true
---

## 树模型简介

树模型(Tree Model)或者说决策树模型(Decision Tree Model)可能是大家在入门机器学习时接触的最早的模型。它在监督学习(supervised learning) 的众多模型中，可以说是最符合人类思维的一种模型：通过一系列的“提问”，最终将这一样本点(sample)归到某个样本集合中去，再通过这一样本集合的共性特征去定义这一样本点的标签(label)。笔者记得很多年前，网络上流行这样的应用：你心中想一个名人明星，系统会给你提问若干问题，你只需要回答“是”、“不是”或者“不知道”即可，经过10+轮的提问，系统能够告诉你心之所想。其实，这个应用的背后就是一个针对名人明星的决策树，只不过这棵决策树不需要泛化能力，完全拟合训练集（名人明星样本）即可。

上述提到的针对样本的“一系列提问”，实际上做的是将整个特征空间(feature space)划分成一个个不相交的矩形(rectangles)。更准确地说，在二维特征空间中是不相交的矩形（如下图1[^1]所示），而在高维空间中是不相交的高维立方体。将样本点归到这些不相交的集合的过程，如下图所示，可以用一种树形的方式描述，这也是模型被称为“树模型”的原因。

{% grouppicture 2-2 %}
  !["树模型的树状描述结构"](/images/trees/tree1.png)
  !["树模型的特征空间划分"](/images/trees/tree2.png)
{% endgrouppicture %}
<div class="image-caption" style="margin: 6 auto;">
  <span class="image-caption" style="margin: 4 auto ">图1：树模型的树状结构描述及特征空间划分</span>
</div>

图2[^1]表示由决策树将`特征空间`划分为不相交的集合且在不同的特征空间子集上的取值，即模型的预测值。如果我们将整个`特征空间`用$\mathcal{R}$来表示，那么决策树会将整个空间划分为$\{\mathcal{R}_1, \mathcal{R}_2, ... , \mathcal{R}_m\}$，其中任意$i\neq j$且$i,j \in {1, ..., m}$满足$\mathcal{R}_i\bigcap\mathcal{R}_j=\emptyset$。

<img src="/images/trees/tree3.png" width="50%" height="50%" border="0" style="margin: 0 auto;"/>
<div class="image-caption" style="margin: 6 auto;">
  <span class="image-caption" style="margin: 4 auto ">图2：树模型特征空间划分及其对应值</span>
</div>

传统的机器学习参考书在介绍树模型时，一般采用的是列举`ID3`、`C4.5`和`CART`这几种树模型的算法。但是，笔者觉得看过一遍这几个算法的介绍，一直很难产生共鸣（这可能也与笔者之前没有真正动手实现一个树模型有关）。在深入阅读参考文献[1]和[3]后，笔者有了更深的感悟，这几种树模型其实核心的逻辑都很类似：我们没有办法暴力穷举去探索所有的`特征空间`的划分，只能够采用贪心策略(greedy strategy)寻找当下的一个最好的划分，而一旦我们划分好`特征空间`，确定每个特征子集合$\mathcal{R}_i$的值的方式都是近似的，这一点将在本文详细介绍。基于这样的认识，笔者认为也许对其中一种树模型详细探讨也许会有更多收获。这里我同参考文献[1]和[3]一样，选择`CART`算法进行详细介绍。工业界现在使用树模型很少会直接使用单棵的决策树拟合模型，而是会采用`Boosting`或者`Bagging`的方法，将很多很多树的结果“综合”一下，以得到更好的预测效果。`CART`的简洁以及既可以用于分类亦可用于回归也让`CART`成为`Boosting`和`Bagging`里广泛使用的基模型[^2]，所以也算是应用最广泛的树模型。

[^1]: 图片摘自参考文献[1]中章节9.2 “Tree-Based Model”。
[^2]: 基模型的概念后续会介绍，这里结合上下文可以理解为“很多树”中的一棵。

## 分类与回归树（Classfication & Regression Tree, CART, C&RT）

CART（部分材料中习惯简写为C&RT）的全称是Classfication And Regression Tree（分类与回归树），顾名思义该算法即可用于`分类`又可用于`回归`。如同之前所述，我们无法用暴力穷举所有的特征空间的划分，只能够采用贪心的方法：我们假设样本特征维度为$d$，特征集合为$\{fea_1,fea_2,...,fea_d\}$，每次针对当前数据集合$\mathcal{D}_{curr}$找到最合适的划分特征${fea}_i$，并依据样本在特征${fea}_i$上的取值将其划分为**两个**数据子集$\mathcal{D}_{next1}$和$\mathcal{D}_{next2}$，然后再采用`递归`的方式处理$\mathcal{D}_{next1}$和$\mathcal{D}_{next2}$这两个数据子集，直到符合终止条件。上述描述既包含树模型训练的基本流程，也包含CART树相对于ID3、C4.5个性特征，具体如下：

1. CART构建的是二叉树，而ID3、C4.5算法则会构建多叉树
> 关于这一点可以参考Hastie在参考文献[1]中9.2节的介绍：采用二叉树能够避免在一次分裂时因为分支过多造成的数据子集$\mathcal{D}_{nexti}$数据量过少，从而减少过拟合的可能。这也是CART算法一个比较“聪明”的一种做法。针对无序的类别特征(categorical feature)，如何将数据集依该特征划分成两个数据子集，笔者将在本篇后续详细介绍。

2. CART在建树的每一层的数据划分时，会考虑所有特征集合
> ID3、C4.5算法在每次分裂时，如果当前结点上样本的特征集合为$\mathcal{FE}$，并且选择当前的分裂特征为${fea}_i$，那么可供该结点的子结点的进行分裂的特征集合为$\mathcal{FE}-{fea}_i$。但是，对于CART算法，每一个结点可分裂的特征集合都是原始的特征集合。用一种更好理解的方式解释这个问题：假设我们希望对朗姆酒是否好喝建立一个分类器，如果用ID3、C4.5算法构建决策树，那么“酒的颜色”这个特征我们只能够用在一次判断上。但是如果是用CART算法建立的决策树，则可能多次用该特征进行判断。

3. CART既可以用于分类问题，又可以用于回归问题
> 这一点在前文中反复提到，在下文将会详细介绍CART分别处理分类问题和回归问题的方式。

### CART中的回归树

&#8194;&#8194;&#8194;&#8194;CART算法用于回归问题最常见的是采用MSE（Mean Square Error，均方误差）作为模型的损失函数，除此之外MAE（Mean Absolute Error，平均绝对误差）也是比较常见的损失函数。假如CART采用MSE损失函数，那么对于结点$\mathcal{N}$，结点分裂需要考虑的：1. 选择什么特征进行分裂？2. 针对特征如何分裂？想回答这两个问题，最重要的是我们必须明确一次分裂能对模型提升什么。

&#8194;&#8194;&#8194;&#8194;对于回归问题，回答刚才的那个问题比较简单，我们不妨这么看：对于结点$\mathcal{N}$，如果我们不继续分裂该结点，那么该结点上的loss为：
$$
L(\mathcal{N}) = \frac{1}{|\mathcal{N}|}\sum_{i, x_i\in\mathcal{N}}(x_i - c(\mathcal{N}))^2
$$
其中，
$$
c(\mathcal{N}) = avg_{x_i\in\mathcal{N}}(y_i)
$$
&#8194;&#8194;&#8194;&#8194;上面的$c(\mathcal{N})$是按照MSE损失函数为结点计算出的预测值，大家很容易通过求导验证$L(\mathcal{N})$当$c(\mathcal{N})$取值为样本均值时，结点上的MSE最小。如果损失函数是MAE时，$c(\mathcal{N})$则取样本中位点值时，损失最小。关于这个结论，我想到的最简单的方法就是画出数轴，模拟切分点的移动观察损失函数的变化，从而得出结论。如果将结点$\mathcal{N}$根据特征${fea}_j$划分为两个子结点$\mathcal{N}_{left}$和$\mathcal{N}_{right}$：
$$
\mathcal{N}_{left}({fea}_j, s)=\{ X \vert X_{fea_j} \leq s \}
$$
$$
\mathcal{N}_{right}({fea}_j, s)=\{ X \vert X_{fea_j} > s \}
$$
那么这两个子结点给出的预测值则分别为：
$$
c(\mathcal{N}_{left}) = avg_{x_i\in\mathcal{N}_{left}}(y_i)
$$
$$
c(\mathcal{N}_{right}) = avg_{x_i\in\mathcal{N}_{right}}(y_i)
$$
这两个结点上的loss则分别为：
$$
L(\mathcal{N}_{left}) = \frac{1}{|\mathcal{N}_{left}|}\sum_{i, x_i\in\mathcal{N}_{left}}(x_i - c(\mathcal{N}_{left}))^2
$$
$$
L(\mathcal{N}_{right}) = \frac{1}{|\mathcal{N}_{right}|}\sum_{i, x_i\in\mathcal{N}_{right}}(x_i - c(\mathcal{N}_{right}))^2
$$
&#8194;&#8194;&#8194;&#8194;一个很自然的想法就是，结点$\mathcal{N}$的loss与两个子结点的loss`加权和`的差为这次分裂所带来的收益。我们定义对于结点$\mathcal{N}$上关于特征的$fea_j$在取值$s$上分裂收益为$G(\mathcal{N}, fea_j, s)$，那么有：
$$
G(\mathcal{N}, fea_j, s)=|\mathcal{N}|L(\mathcal{N})-(|\mathcal{N}_{left}|L(\mathcal{N}_{left}) + |\mathcal{N}_{right}|L(\mathcal{N}_{right}))
$$
&#8194;&#8194;&#8194;&#8194;那么对于特征$fea_j$，我们如何找到最为合适的$s$？对于这个问题，一般采取的策略是将结点$\mathcal{N}$上的数据按照特征$fea_j$的值进行排序，每两个相邻的数据点的平均值$s_i = \frac{1}{2}(x_i+x_{i+1})$作为候选的分裂阈值，并依此评估分裂收益$G(\mathcal{N}, fea_j, s_i)$。这样对于每个特征$fea_j$都能够找到一个最大的收益$G(\mathcal{N}, fea_j)$。图3表示针对一个特征对数据进行排序，并遍历不同的分裂阈值$\{s_1,s_2,...,s_8\}$，并找到最合适的分裂阈值$s_4$。

<img src="/images/trees/tree4.png" width="90%" height="90%" border="0" style="margin: 0 auto;"/>
<div class="image-caption" style="margin: 6 auto;">
  <span class="image-caption" style="margin: 4 auto ">图3：寻找特征的最佳分裂阈值</span>
</div>

&#8194;&#8194;&#8194;&#8194;针对一个特征$fea_j$，我们按照上述方法找到最合适的分裂阈值，并能够得到在该特征下的最大分裂收益$G(\mathcal{N}, fea_j)$。CART算法计算所有的特征下的最大分裂收益，并从中选择最大分裂收益最大值对应的特征进行分裂。
&#8194;&#8194;&#8194;&#8194;至此我们明白了在所有的特征值都为连续值的情况下，如何找到最合适的分裂特征以及对应的分裂阈值。那么，对于特征值是离散情况，我们如何处理呢？


### CART中的分类树

## 漏掉的细节

## CART与ID3、C4.5的对比

CART与ID3、C4.5的对比在前文也陆陆续续地介绍了一些，这里总结一下：

- CART算法既能用于分类问题，又能用于回归问题；ID3、C4.5算法只能用于分类问题
- CART算法构建的树为二叉树；ID3、C4.5算法构建的是多叉树
- ID3只能处理特征值为离散特征的数据，C4.5算法能够处理连续值特征[^3]，CART算法既能处理离散特征又能处理连续特征

[^3]: 不同文献介绍的内容存在差异，大致提到C4.5能够将连续特征值分段，但是是否采用与CART类似的排序二分目前还没搞清楚。

## 决策树的优缺点

## 参考文献

[1]: Friedman J, Hastie T, Tibshirani R. The elements of statistical learning[M]. New York: Springer series in statistics, 2001.
[2]: 李航. 统计学习方法[J]. 2019.
[3]: David S. Rosenberg. https://bloomberg.github.io
[4]: 诸葛越, 葫芦娃. 百面机器学习——算法工程师带你去面试[J]. 2018.
