---
title: （三）树模型(Decision Trees)
date: 2019-12-05 19:17:38
tags: [树, 决策树, 回归树, Decision Tree]
mathjax: true
---

## 树模型简介

树模型(Tree Model)或者说决策树模型(Decision Tree Model)可能是大家在入门机器学习时接触的最早的模型。它在监督学习(supervised learning) 的众多模型中，可以说是最符合人类思维的一种模型：通过一系列的“提问”，最终将这一样本点(sample)归到某个样本集合中去，再通过这一样本集合的共性特征去定义这一样本点的标签(label)。笔者记得很多年前，网络上流行这样的应用：你心中想一个名人明星，系统会给你提问若干问题，你只需要回答“是”、“不是”或者“不知道”即可，经过10+轮的提问，系统能够告诉你心之所想。其实，这个应用的背后就是一个针对名人明星的决策树，只不过这棵决策树不需要泛化能力，完全拟合训练集（名人明星样本）即可。

上述提到的针对样本的“一系列提问”，实际上做的是将整个特征空间(feature space)划分成一个个不相交的矩形(rectangles)。更准确地说，在二维特征空间中是不相交的矩形（如下图1[^1]所示），而在高维空间中是不相交的高维立方体。将样本点归到这些不相交的集合的过程，如下图所示，可以用一种树形的方式描述，这也是模型被称为“树模型”的原因。

{% grouppicture 2-2 %}
  !["树模型的树状描述结构"](/images/trees/tree1.png)
  !["树模型的特征空间划分"](/images/trees/tree2.png)
{% endgrouppicture %}
<div class="image-caption" style="margin: 6 auto;">
  <span class="image-caption" style="margin: 4 auto ">图1：树模型的树状结构描述及特征空间划分</span>
</div>

图2[^1]表示由决策树将`特征空间`划分为不相交的集合且在不同的特征空间子集上的取值，即模型的预测值。如果我们将整个`特征空间`用$\mathcal{R}$来表示，那么决策树会将整个空间划分为$\{\mathcal{R}_1, \mathcal{R}_2, ... , \mathcal{R}_m\}$，其中任意$i\neq j$且$i,j \in {1, ..., m}$满足$\mathcal{R}_i\bigcap\mathcal{R}_j=\emptyset$。

<img src="/images/trees/tree3.png" width="50%" height="50%" border="0" style="margin: 0 auto;"/>
<div class="image-caption" style="margin: 6 auto;">
  <span class="image-caption" style="margin: 4 auto ">图2：树模型特征空间划分及其对应值</span>
</div>

传统的机器学习参考书在介绍树模型时，一般采用的是列举`ID3`、`C4.5`和`CART`这几种树模型的算法。但是，笔者觉得看过一遍这几个算法的介绍，一直很难产生共鸣（这可能也与笔者之前没有真正动手实现一个树模型有关）。在深入阅读参考文献[1]和[3]后，笔者有了更深的感悟，这几种树模型其实核心的逻辑都很类似：我们没有办法暴力穷举去探索所有的`特征空间`的划分，只能够采用贪心策略(greedy strategy)寻找当下的一个最好的划分，而一旦我们划分好`特征空间`，确定每个特征子集合$\mathcal{R}_i$的值的方式都是近似的，这一点将在本文详细介绍。基于这样的认识，笔者认为也许对其中一种树模型详细探讨也许会有更多收获。这里我同参考文献[1]和[3]一样，选择`CART`算法进行详细介绍。工业界现在使用树模型很少会直接使用单棵的决策树拟合模型，而是会采用`Boosting`或者`Bagging`的方法，将很多很多树的结果“综合”一下，以得到更好的预测效果。`CART`的简洁以及既可以用于分类亦可用于回归也让`CART`成为`Boosting`和`Bagging`里广泛使用的基模型[^2]，所以也算是应用最广泛的树模型。

[^1]: 图片摘自参考文献[1]中章节9.2 “Tree-Based Model”。
[^2]: 基模型的概念后续会介绍，这里结合上下文可以理解为“很多树”中的一棵。

## 分类与回归树（Classfication & Regression Tree, CART, C&RT）

CART（部分材料中习惯简写为C&RT）的全称是Classfication And Regression Tree（分类与回归树），顾名思义该算法即可用于`分类`又可用于`回归`。如同之前所述，我们无法用暴力穷举所有的特征空间的划分，只能够采用贪心的方法：我们假设样本特征维度为$d$，特征集合为$\{fea_1,fea_2,...,fea_d\}$，每次针对当前数据集合$\mathcal{D}_{curr}$找到最合适的划分特征${fea}_i$，并依据样本在特征${fea}_i$上的取值将其划分为**两个**数据子集$\mathcal{D}_{next1}$和$\mathcal{D}_{next2}$，然后再采用`递归`的方式处理$\mathcal{D}_{next1}$和$\mathcal{D}_{next2}$这两个数据子集，直到符合终止条件。上述描述既包含树模型训练的基本流程，也包含CART树相对于ID3、C4.5个性特征，具体如下：

1. CART构建的是二叉树，而ID3、C4.5算法则会构建多叉树
> 关于这一点可以参考Hastie在参考文献[1]中9.2节的介绍：采用二叉树能够避免在一次分裂时因为分支过多造成的数据子集$\mathcal{D}_{nexti}$数据量过少，从而减少过拟合的可能。这也是CART算法一个比较“聪明”的一种做法。针对无序的类别特征(categorical feature)，如何将数据集依该特征划分成两个数据子集，笔者将在本篇后续详细介绍。

2. CART在建树的每一层的数据划分时，会考虑所有特征集合
> ID3、C4.5算法在每次分裂时，如果当前结点上样本的特征集合为$\mathcal{FE}$，并且选择当前的分裂特征为${fea}_i$，那么可供该结点的子结点的进行分裂的特征集合为$\mathcal{FE}-{fea}_i$。但是，对于CART算法，每一个结点可分裂的特征集合都是原始的特征集合。用一种更好理解的方式解释这个问题：假设我们希望对朗姆酒是否好喝建立一个分类器，如果用ID3、C4.5算法构建决策树，那么“酒的颜色”这个特征我们只能够用在一次判断上。但是如果是用CART算法建立的决策树，则可能多次用该特征进行判断。

3. CART既可以用于分类问题，又可以用于回归问题
> 这一点在前文中反复提到，在下文将会详细介绍CART分别处理分类问题和回归问题的方式。

### CART中的分类树

### CART中的回归树

### CART里的细节

## CART与ID3、C4.5的对比

## 参考文献

[1]: Friedman J, Hastie T, Tibshirani R. The elements of statistical learning[M]. New York: Springer series in statistics, 2001.
[2]: 李航. 统计学习方法[J]. 2019.
[3]: David S. Rosenberg. https://bloomberg.github.io
[4]: 诸葛越, 葫芦娃. 百面机器学习——算法工程师带你去面试[J]. 2018.
