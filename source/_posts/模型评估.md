---
title: （二）模型评估
date: 2019-12-04 00:44:11
tags: [Model Evaluation, 模型评估]
mathjax: true
---

如何评估一个模型是好是坏？在分类问题中，我们最常听说的PR曲线是什么？我们听到的Roc曲线又是什么呢？到底我们是该用Roc曲线还是PR曲线去衡量一个二分类模型的好坏呢？本文在介绍模型评估的基本概念的同时，也将回答上述问题。

<!-- more -->

## 精确度(Precision)、召回率(Recall) 与 准确率(Accuracy)

![](/images/pr/pr1.png)
<div class="image-caption" style="margin: 6 auto;">
  <span class="image-caption" style="margin: 4 auto ">图1：精确度、召回率和准确率</span>
</div>

&#8194;&#8194;&#8194;&#8194;精确度(Precision)和召回率(Recall)用来评估二分类模型，精确度(Accuracy)既可以用来评估二分类模型的效果，也可以评估多分类模型。其中，精确度(Precision)的定义为：$Precision=\frac{TP}{TP+FP}$，也就是我们用训练好的二分类模型判断出数据集（这里的数据集指验证集，专门用来评估模型是好是坏的带有label的数据集）的正样本中真实的正样本所占的比例。召回率(Recall)的定义为：$Recall=\frac{TP}{TP+FN}$，也就是模型分类结果中标为正样本的sample个数占数据集中真实正样本个数的比例。准确率(Accuracy)的概念相对更加直接：$Accuracy=\frac{TP+TN}{TP+TN+FP+FN}$，也就是说分类器对数据集分类正确的样本占样本全集的比例。大家也可以参照图1理解这三个概念。

&#8194;&#8194;&#8194;&#8194;在学校学习的时候或者在Coursera、Edx、学堂在线这些平台上看一些公开课的时候，用的比较多的是准确率(Accuracy)这个概念，但是本人在“算法工程师”的日常工作中，用到的更多的却是精确度(Precision)和召回率(Recall)这两个概念。这是为什么呢？我的理解如下：

1. 工业界实际数据与courses或者papers的数据集存在差异：在课程或者论文中的数据集，很大一部分都是样本均衡的数据集，因为我们focus在模型上，而非具体的数据，也不必完全贴近真实场景下的数据分布。举个具体的例子，我在学习深度学习图像识别的时候，评估CNN模型一般都是使用CIFA10或者ImageNet这样非常“标准”的数据集，CIFA10中包含10个类别（当然也确实没法直接使用精确度和召回率来评估，除非用one-vs-rest这种方法），每个类别下的样本个数基本一致，此时准确率(Accuracy)已经完全能够评估模型是好是坏。而在工业场景中，例如转化率预估场景，数据集的正负样本差异极大，这时候如果直接用准确率评估则毫无意义。假如点击率预估数据集中正样本比例为$0.2\%$，负样本比例为$99.8\%$，那么即使模型把所有的样本都预测为负，模型的准确率也能达到$99.8\%$。在这个场景下谈准确率意义不大。

2. 模型的精确度、召回率亦或是准确率都和我们设置的阈值有关（模型会给出一个待预测sample为正的概率，最终判断该样本为正为负通过判断概率值与阈值的大小关系进行），在工业界中一般都会结合具体业务场景去决策阈值的设置：比如一个人脸身份验证系统，我们往往需要保证系统的可靠性，这时候往往需要通过设置较高的阈值，保证“验证通过”这样的正样本具有很高的精确度。再比如，如果你要设计一个自动识别bug的系统，这时候对系统的要求则可能是尽可能找到所有的bug，此时需要设置相对较低的阈值以保证识别结果的高召回。

&#8194;&#8194;&#8194;&#8194;我自己在日常“算法工程师”的工作中，一般的工作流程是，对训练好的模型画出它的PR曲线（下面将会介绍），结合业务需要在这个PR曲线上找到一个合适的点（即确定阈值），评估模型的准确率使用相对低频。

## PR曲线和Roc曲线

### PR曲线

&#8194;&#8194;&#8194;&#8194;PR曲线的全称是Precision-Recall曲线，顾名思义组成该曲线的点的坐标由`召回率`和`精确率`组成。在学习的逻辑斯蒂回归(Logistic Regression)时，我们知道逻辑斯蒂函数（Logistic Function，函数形式为$f(x)=\frac{1}{1+e^{-x}}$）将会计算出一个位于$(0,1)$区间之内的值，并且将该值作为样本归属正样本集的概率值$p(x=1)$。但正如前文所说，我们要视情况决定我们采取什么样的`判断阈值`threshold（当$p(x)\geq threshold$时，将样本预测为正样本，反之当$p(x)<threshold$时，将样本预测为负样本）。所以，我们需要看模型在不同的阈值下的精确和召回情况。PR曲线一般以召回率作为横轴，精确率作为纵轴绘制，刻画出模型在不同的召回情况下的精确程度。

&#8194;&#8194;&#8194;&#8194;PR曲线一方面让模型在不同召回率下的精确度一眼就看出来，另一方面使得模型对比更加充分。图2[^1]中包含模型A和模型B两个模型的准召曲线，可以看出当召回比较少时模型B的精确度能达到1.0，要与模型A在召回较少时的精确度。但当召回较大时模型B的精确度指标却弱于模型A。这说明仅仅取一个$(precision, recall)$点来评估模型是不够的，从PR曲线分析模型更能全面了解模型在不同的召回下的表现。

<img src="/images/pr/pr2.png" width="70%" height="70%" border="0" style="margin: 0 auto;"/>
<div class="image-caption" style="margin: 6 auto;">
  <span class="image-caption" style="margin: 4 auto ">图2：两个模型的准召曲线</span>
</div>

[^1]: 图片来自参考文献[1]中章节2 “模型评估”。

### ROC曲线

&#8194;&#8194;&#8194;&#8194;ROC曲线的全称是`受试者工作特征曲线`(Receiver Operating Characteristic Curve)，它的横轴是`假阳性率`(False Positive Ratio, FPR)，纵轴为`真阳性率`(True Positive Ratio, TPR)。所谓假阳性率是指被分类器预测为正样本但实际上是负样本的个数占总体负样本个数的比例，而真阳性率则表示被分类器预测为正样本且实际是正样本的个数占总体正样本个数的比例。为了便于理解，大家可以借助图1来理解，`真阳性率`为$\frac{TP}{TP+FN}$，实际上就是我们PR曲线的召回率，`假阳性率`为$\frac{FP}{FP+TN}$。绘制ROC曲线和绘制PR曲线一样，通过给模型设定不同的阈值，得到不同的$(FPR,TPR)$的坐标点，进而绘制出ROC曲线。图3[^2]即为根据该方法绘制而成的ROC曲线。

<img src="/images/pr/pr3.png" width="70%" height="70%" border="0" style="margin: 0 auto;"/>
<div class="image-caption" style="margin: 6 auto;">
  <span class="image-caption" style="margin: 4 auto ">图3：模型的ROC曲线</span>
</div>

[^2]: 图片来自参考文献[1]中章节2 “模型评估”。

### 什么时候用PR曲线，什么时候用ROC曲线？

&#8194;&#8194;&#8194;&#8194;为了回答这个问题，大家需要知道ROC曲线的一个重要的性质：当验证集的正负样本比例发生变化时，ROC曲线的形状能够基本保持不变。PR曲线在这样的情况下曲线变化比较剧烈。图4[^3]中，在验证集的正负样本比发生剧烈变化时，模型A和模型B的ROC曲线基本保持一致，但PR曲线变化则非常明显：

<img src="/images/pr/pr4.png" width="100%" height="100%" border="0" style="margin: 0 auto;"/>
<div class="image-caption" style="margin: 6 auto;">
  <span class="image-caption" style="margin: 4 auto ">图4：不同样本比下的ROC曲线和PR曲线</span>
</div>

&#8194;&#8194;&#8194;&#8194;笔者在日常工作中训练的模型在评估时也印证了这一点，图5中第一排两张图片表示模型在两个不同正负样本比的验证集下的PR曲线：虽然曲线的形状近似，但是观察坐标轴的刻度可以看出两条曲线的差异较为明显；而图5第二排的两张图片表示模型在这两个验证集下的ROC曲线：曲线无论从形状还是点的取值来说，基本上保持一致。

{% grouppicture 4-3 %}
  ![](/images/pr/pr5.png)
  ![](/images/pr/pr6.png)
  ![](/images/pr/pr7.png)
  ![](/images/pr/pr8.png)
{% endgrouppicture %}
<div class="image-caption" style="margin: 6 auto;">
  <span class="image-caption" style="margin: 4 auto ">图5：不同验证集上的ROC曲线和PR曲线</span>
</div>

因此，我们可以得到这样的结论：
> 当我们需要评估模型在不同验证集上的效果，尤其是样本比差异较大的验证集上，选择ROC能够得到更加稳定的效果
> 当我们需要仔细地观察模型在某个特定验证集上的效果，选择PR曲线更加合适，因为PR曲线相对更加直观

&#8194;&#8194;&#8194;&#8194;这里顺便提一句，我们称ROC曲线下的面积值为`AUC`，也即对ROC曲线在$(0,1)$上求积分得到的结果。AUC是一个量化衡量ROC曲线好坏的标准，它的取值在区间$(0.5,1)$。AUC值越大，模型的效果越好。


[^3]: 图片来自参考文献[1]中章节2 “模型评估”。

## 参考文献

[1]: 诸葛越, 葫芦娃. 百面机器学习——算法工程师带你去面试[J]. 2018.
